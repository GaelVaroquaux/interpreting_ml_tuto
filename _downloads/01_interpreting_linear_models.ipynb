{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nInterpreting linear models\n==========================\n\nLinear models are not that easy to interpret when variables are\ncorrelated.\n\nSee also the `statistics chapter\n<http://www.scipy-lectures.org/packages/statistics/index.html>`_ of the\n`scipy lecture notes <http://www.scipy-lectures.org>`_\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Data on wages\n--------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import urllib\nimport os\nimport pandas\n\nif not os.path.exists('wages.txt'):\n    # Download the file if it is not present\n    urllib.urlretrieve('http://lib.stat.cmu.edu/datasets/CPS_85_Wages',\n                       'wages.txt')\n\n# Give names to the columns\nnames = [\n    'EDUCATION: Number of years of education',\n    'SOUTH: 1=Person lives in South, 0=Person lives elsewhere',\n    'SEX: 1=Female, 0=Male',\n    'EXPERIENCE: Number of years of work experience',\n    'UNION: 1=Union member, 0=Not union member',\n    'WAGE: Wage (dollars per hour)',\n    'AGE: years',\n    'RACE: 1=Other, 2=Hispanic, 3=White',\n    'OCCUPATION: 1=Management, 2=Sales, 3=Clerical, 4=Service, 5=Professional, 6=Other',\n    'SECTOR: 0=Other, 1=Manufacturing, 2=Construction',\n    'MARR: 0=Unmarried,  1=Married',\n]\n\nshort_names = [n.split(':')[0] for n in names]\n\ndata = pandas.read_csv('wages.txt', skiprows=27, skipfooter=6, sep=None,\n                       header=None)\ndata.columns = short_names\n\n# Log-transform the wages, as they typically increase with\n# multiplicative factors\nimport numpy as np\ndata['WAGE'] = np.log10(data['WAGE'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The challenge of correlated features\n--------------------------------------------\n\nPlot scatter matrices highlighting different aspects\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import seaborn\nseaborn.pairplot(data, vars=['WAGE', 'AGE', 'EDUCATION', 'EXPERIENCE'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note that age and experience are highly correlated\n\nA link between a single feature and the target is a *marginal* link.\n\n\nUnivariate feature selection selects on marginal links.\n\nLinear model compute *conditional* links: removing the effects of other\nfeatures on each feature. This is hard when features are correlated.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Coefficients of a linear model\n--------------------------------------------\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import linear_model\nfeatures = [c for c in data.columns if c != 'WAGE']\nX = data[features]\ny = data['WAGE']\nridge = linear_model.RidgeCV()\nridge.fit(X, y)\n\n# Visualize the coefs\ncoefs = ridge.coef_\nfrom matplotlib import pyplot as plt\nplt.barh(np.arange(coefs.size), coefs)\nplt.yticks(np.arange(coefs.size), features)\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note: coefs cannot easily be compared if X is not standardized: they\nshould be normalized to the variance of X.\n\nWhen features are not too correlated and their is plenty, this is the\nwell-known regime of standard statistics in linear models. Machine\nlearning is not needs, and statsmodels is a great tool (see the\nstatistics chapter in scipy-lectures)\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The effect of regularization\n--------------------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "lasso = linear_model.LassoCV()\nlasso.fit(X, y)\n\ncoefs = lasso.coef_\nfrom matplotlib import pyplot as plt\nplt.barh(np.arange(coefs.size), coefs)\nplt.yticks(np.arange(coefs.size), features)\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "l1 regularization (sparse models) puts variables to zero.\n\nWhen two variables are very correlated, it will put arbitrary one or\nthe other to zero depending on their SNR. Here we can see that age\nprobably overshadowed experience.\n\n_____________________\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.14", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}