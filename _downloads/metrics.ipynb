{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nMetrics to judge the sucess of a model\n=======================================\n\nPro & cons of various performance metrics.\n\nThe simple way to use a scoring metric during cross-validation is\nvia the `scoring` parameter of\n:func:`sklearn.model_selection.cross_val_score`.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Regression settings\n-----------------------\n\nThe Boston housing data\n........................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import datasets\nboston = datasets.load_boston()\n\n# Shuffle the data\nfrom sklearn.utils import shuffle\ndata, target = shuffle(boston.data, boston.target, random_state=0)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "A quick plot of how each feature is related to the target\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from matplotlib import pyplot as plt\n\nfor feature, name in zip(data.T, boston.feature_names):\n    plt.figure(figsize=(4, 3))\n    plt.scatter(feature, target)\n    plt.xlabel(name, size=22)\n    plt.ylabel('Price (US$)', size=22)\n    plt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We will be using a random forest regressor to predict the price\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Explained variance vs Mean Square Error\n.......................................\n\nThe default score is explained variance\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import cross_val_score\nprint(cross_val_score(regressor, data, target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Explained variance is convienent because it has a natural scaling: 1 is\nperfect prediction, and 0 is around chance\n\nNow let us see which houses are easier to predict:\n\nNot along the Charles river (feature 3)\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 0],\n                      target[data[:, 3] == 0]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So the houses along the Charles are harder to predict?\n\nIt's not so easy to conclude this from the explained variance: in two\ndifferent sets of observations, the variance of the target differs, and\nthe explained variance is a relative measure\n\n**MSE**: We can use the mean squared error (here negated)\n\nNot along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 0],\n                      target[data[:, 3] == 0],\n                      scoring='neg_mean_squared_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1],\n                      scoring='neg_mean_squared_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So the error is larger along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Mean Squared Error versus Mean Absolute Error\n..................................................\n\nWhat if we want to report an error in dollars, meaningful for an\napplication?\n\nThe Mean Absolute Error is useful for this goal\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data, target,\n                      scoring='neg_mean_absolute_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Summary\n.........\n\n* **explained variance**: scaled with regards to chance: 1 = perfect,\n  0 = around chance, but it shouldn't used to compare predictions\n  across datasets\n\n* **mean absolute error**: enables comparison across datasets in the\n  units of the target\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Classification settings\n-----------------------\n\nThe digits data\n.................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "digits = datasets.load_digits()\n# Let us try to detect sevens:\nsevens = (digits.target == 7)\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Accuracy and its shortcomings\n.............................\n\nThe default metric is the accuracy: the averaged fraction of success.\nIt takes values between 0 and 1, where 1 is perfect prediction\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "However, a stupid classifier can each good prediction wit imbalanced\nclasses\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.dummy import DummyClassifier\ndummy = DummyClassifier(strategy='most_frequent')\nprint(cross_val_score(dummy, digits.data, sevens))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Balanced accuracy (available in development scikit-learn versions)\nfixes this, but can have surprising behaviors, such as being negative\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Precision, recall, and their shortcomings\n..........................................\n\nWe can measure separately false detection and misses\n\n**Precision**: Precision counts the ratio of detections that are\ncorrect\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens,\n                      scoring='precision'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Our classifier has a good precision: most of the sevens that it\npredicts are really sevens.\n\nAs predicting the most frequent never predicts sevens, precision is ill\ndefined. Scikit-learn puts it to zero\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(dummy, digits.data, sevens, scoring='precision'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**Recall**: Recall counts the fraction of class 1 actually detected\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens, scoring='recall'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Our recall isn't as good: we miss many sevens\n\nBut predicting the most frequent never predicts sevens:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(dummy, digits.data, sevens, scoring='recall'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**Note** Measuring only the precision without the recall makes no\nsense, it is easy to maximize one at the cost of the other. Ideally,\nclassifiers should be compared on a precision at a given recall\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Area under the ROC curve\n..........................\n\nIf the classifier provides a decision function that can be thresholded\nto control false positives versus false negatives, the ROC curve\nsummarise the different tradeoffs that can be achieved by varying this\nthreshold.\n\nIts Area Under the Curve (AUC) is a useful metric where 1 is perfect\nprediction and .5 is chance, independently of class imbalance\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens, scoring='roc_auc'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Average precision\n..................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Multiclass and multilabel settings\n...................................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.14", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}